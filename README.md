# Mini GPT in Java

This repository contains a simplified implementation of a Generative Pre-trained Transformer (GPT) model using Java. The project demonstrates the core concepts behind the GPT model, including the self-attention mechanism and Transformer architecture, using the Deep Java Library (DJL) and MXNet.

## Overview

GPT (Generative Pre-trained Transformer) models are widely used in natural language processing (NLP) for tasks such as text generation, summarization, and translation. This project provides a minimal implementation of a GPT model in Java, focusing on the essential components like self-attention and basic Transformer blocks.

### Key Features:
- Implementation of a mini GPT model using DJL and MXNet.
- Self-attention mechanism for capturing relationships between tokens.
- Simple training loop with dummy data.
- Text generation example with a pre-trained model.

## Requirements

- **Java Development Kit (JDK) 11 or later**
- **Apache Maven** (for managing dependencies)
- **Deep Java Library (DJL)**: A Java library for deep learning.
- **MXNet**: The deep learning engine used by DJL.

## Installation

1. **Clone the repository:**

   ```bash
   git clone https://github.com/your-username/Mini_GPT.git
   bash```

   ```bash
   cd Mini_GPT
   bash```
